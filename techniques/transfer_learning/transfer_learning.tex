\subsection{Transfer Learning}
\label{sec:transfer_learning}

Coming back to the human learning analogy, it is often the case that knowledge or skills gained in one field can be applied to other fields, without having a lot of experience in the new fields. For example one might take their knowledge about composition from drawing to apply it to taking good pictures in photography. Although the technical details of both fields are different, they have some shared concepts, which help a person proficient in one to perform well in the other. That same person might also be able to improve in this new discipline faster than others, who do not have any applicable prior knowledge, because they can focus on learning other key skills needed to excel.

\emph{Transfer leaning} is the idea to apply this concept to neural network training. From a high level perspective, what happens during computation in a neural network, is that each layer transforms the input into an \emph{intermediate representation} of the data, which extracts different features present in the input. For example, one layer might learn to detect edges, which the next layer then combines to knowledge about the location of corners. As you can imagine, these types of features are useful for recognizing objects, such as a tennis ball, in a picture. It also stands to reason that these same features would be useful to detect circular droplets. The extracted features are rarely this clear cut or human-understandable as in this example, but the point still stands. 

Using a model trained on one dataset and using these weights as a starting point to fine tune the model on the actual problem dataset is also called \emph{pre-training}. This is especially useful if the data for the target problem is scarce, as pretraining is often done on very large datasets such as \emph{ImageNet}, which features over 14 million images (smaller subsets available) annotated for \emph{image classification}. Pretrained weigths for popular architectures are readily available for download and can be used as is if the output format of the network fits the new data, by switching out the final layers and learning only the classification or by using them as part of a bigger network, for example using them as a feature extractor in an \emph{encoder-decoder} architecture (see \ref{sec:architectures})

There are other forms of transfer learning, such as \emph{distillation}, where the concept is to use a complex model that is well adapted to the task to train a comparatively smaller model. The larger model has a higher knowledge capacity, but not all of this capacity has learned important knowledge. When training the smaller model on the soft outputs (before argmax) of the larger model, it may learn correlations which it might not have been able to learn on its own given its limited capacity. The smaller network could then be deployed instead of the larger one to save computation resources on weaker hardware.

In this thesis pre-training is used in two places. Firstly, for the encoder module of the U-Net architecture, a ResNet that is pretrained on the ImageNet dataset is used. Secondly, the experiments examine if pretraining the model on the Cityscapes dataset helps to improve performance on the vapour image dataset.