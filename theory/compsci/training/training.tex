\subsection{Training neural networks}
\label{sec:training}

As mentioned previously neural networks with approppriate weights can be used to approximate any mathematical mapping between two Euclidean spaces (and more), but which weights are approppriate is not at all apparent for any nontrivial task. Similarly how humans don't immediately perform to their best potential at a new task, but can train the brain to improve at solving a particular problem by strengthening the corresponding nervous pathways through repeated exercise, a neural network can \emph{learn} the correct weights for a task in a process called \emph{neural network training}.

Training a neural network requires annotated data for a problem, where the ground truth for the problem at hand is known and encoded in a way that is compatible with the output format of the neural network that should be trained. Keeping in line with the topic of this thesis, for a pixel level semantic segmentation task, this would be in the form of segmentation masks where each pixel has the id of the correct class as its value. 

Then, training happens in several steps. First, a sample from the training data is taken and the model, typically initialized with (semi-) random weights, computes its prediction for the problem (a segmentation mask). Secondly, a \emph{loss function} is applied to measure a distance between the prediction and the ground truth in the solution space. There are several functions which can be used as a loss function for one particular problem which may or may not perform better than others in expressing how wrong the model was with its prediction. Next the contribution of each learnable parameter to the loss is computed by propagating the error backwards through the network (\emph{backpropagation}) and the parameters are then very slightly adjusted to reduce that error. There are several differen rules for updating the weights in each step, but widely used procedures include \emph{stochastic gradient descent (SGD)} and \emph{adam}. 

There are several parameters that influence how training progresses or even how the model operates, but aren't learnable parameters like the weights of the model. These parameters are called \emph{hyperparameters}. This includes the \emph{learning rate} which influences how much the parameters get updated in each step, but also things like the kernel size of a convolutional layer, although these kinds of structural parameters mostly belong to the \emph{architecture} aspect of neural network training and aren't changed as readily. Most of the time, samples aren't processed one by one, but stacked up in \emph{batches}, whose size (the \emph{batch size}) constitute another important hyperparameter. Processing multiple samples at once can be more efficient and also make the training process more stable by presenting more varied data to the network during each optimization step. Batch size is also limited by the size of the training samples in relation to the memory of the hardware used, so it can't be made arbitrarily large. 

What hyperparameters are used during training can have a huge impact on final model performance, so finding optimal hyperparameters is vital when developing a model. The aproppriate hyperparameters depend a lot on the training data and model architecture and are largely determined through trial and error. There are systematic approaches to hyperparameter tuning, but these are often very computationally expensive, since they perform a lot of trial runs. For simpler problems it is often enough to perform the search manually, by starting at values which are known to be in the correct order of magnitude for similar problems and then make informed adjustements based on the results of fewer trial runs. 

Another important part of model training is \emph{data augmentation}, which means to apply random perturbations to the inputs or even the model itself (in the case of \emph{dropouts}) during training to prevent the model from \emph{overfitting}. This refers to the model \emph{memorizing} the training data instead of learning the concept of the problem, leading to high performance on the training data at the cost of worse performance on unseen data, which is undesirable in most cases. The noise applied by augmentation reduces the likelyhood of this happening and generally improves model performance. Since the augmentations are random, they functionally produce additional samples, which can help especially in cases with little training data. Some typical augmentations for computer vision tasks include \emph{random horizontal/vertical flipping}, \emph{scaling and resizing} or \emph{applying gauss noise}. 